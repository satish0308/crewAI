{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'myenv (Python 3.12.4)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from crewai import Crew,Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'myenv (Python 3.12.4)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFaceEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'myenv (Python 3.12.4)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'myenv (Python 3.12.4)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "os.getenv('HF_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'myenv (Python 3.12.4)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "os.environ[\"HUGGINGFACE_ACCESS_TOKEN\"] =os.getenv('HF_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'myenv (Python 3.12.4)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from agents import blog_researcher,blog_writer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'myenv (Python 3.12.4)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from task import research_task,writng_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'myenv (Python 3.12.4)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#pip install langchain_huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'myenv (Python 3.12.4)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#pip install --upgrade --quiet  langchain sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'myenv (Python 3.12.4)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#pip install crewai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'myenv (Python 3.12.4)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#pip install python_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'myenv (Python 3.12.4)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'myenv (Python 3.12.4)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "DATA=os.getenv('HF_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'myenv (Python 3.12.4)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'myenv (Python 3.12.4)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from crewai import Crew,Process\n",
    "from tools import yt_tool\n",
    "from agents import blog_researcher,blog_writer\n",
    "from task import research_task,writng_task\n",
    "import os\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "os.environ[\"HUGGINGFACE_ACCESS_TOKEN\"] =os.getenv('HF_KEY')\n",
    "# os.environ[\"OPENAI_API_KEY\"]=\"\"\n",
    "# # os.environ[\"OPENAI_MODEL_NAME\"]='llama3-8b-8192'\n",
    "# # os.environ[\"OPENAI_API_BASE\"]=\"https://api.groq.com/openai/v1\"\n",
    "# from langchain_community.llms import HuggingFaceEndpoint\n",
    "\n",
    "# llm = HuggingFaceEndpoint(\n",
    "#     endpoint_url=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "#     huggingfacehub_api_token=os.getenv('HF_KEY'),\n",
    "#     task=\"text-generation\",\n",
    "#     max_new_tokens=512\n",
    "# )\n",
    "#os.environ['OPENAI_API_BASE']='http://localhost:11434'\n",
    "#os.environ['OPENAI_MODEL_NAME']='llama3.1'  # Adjust based on available model\n",
    "#os.environ['OPENAI_API_KEY']='' # No API Key required for Ollama\n",
    "\n",
    "\n",
    "# llm = ChatOllama(\n",
    "#     model = \"llama3.1\",\n",
    "#     base_url = \"http://localhost:11434\")\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model = \"llama2\",\n",
    "    base_url = \"http://localhost:11434\",\n",
    "    api_key=\"NA\")\n",
    "\n",
    "crew=Crew(\n",
    "    agents=[blog_researcher,blog_writer],\n",
    "    tasks=[research_task,writng_task],\n",
    "    #process=Process.sequential,  # Optional: Sequential task execution is default\n",
    "    full_output=True,\n",
    "    memory=True,\n",
    "    cache=True,\n",
    "    max_rpm =100,\n",
    "    share_crew=True,\n",
    "    verbose=True,\n",
    "    manager_llm=llm,\n",
    "    planning_llm=llm,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'myenv (Python 3.12.4)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "llm.invoke(\"what is generative AI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m [2024-09-04 11:02:14][DEBUG]: == Working Agent: Blog Researcher from YouTube Videos\u001b[00m\n",
      "\u001b[1m\u001b[95m [2024-09-04 11:02:14][INFO]: == Starting Task: Identify the video AI vs ML vs DL vs Data Science Get detiailed information about the video from the channel\u001b[00m\n",
      "Connection error. <module 'sys' (built-in)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\satish.hiremath\\Documents\\GitHub\\crewAI\\myenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 72, in map_httpcore_exceptions\n",
      "    yield\n",
      "  File \"c:\\Users\\satish.hiremath\\Documents\\GitHub\\crewAI\\myenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 236, in handle_request\n",
      "    resp = self._pool.handle_request(req)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\satish.hiremath\\Documents\\GitHub\\crewAI\\myenv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 216, in handle_request\n",
      "    raise exc from None\n",
      "  File \"c:\\Users\\satish.hiremath\\Documents\\GitHub\\crewAI\\myenv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 196, in handle_request\n",
      "    response = connection.handle_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\satish.hiremath\\Documents\\GitHub\\crewAI\\myenv\\Lib\\site-packages\\httpcore\\_sync\\connection.py\", line 99, in handle_request\n",
      "    raise exc\n",
      "  File \"c:\\Users\\satish.hiremath\\Documents\\GitHub\\crewAI\\myenv\\Lib\\site-packages\\httpcore\\_sync\\connection.py\", line 76, in handle_request\n",
      "    stream = self._connect(request)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\satish.hiremath\\Documents\\GitHub\\crewAI\\myenv\\Lib\\site-packages\\httpcore\\_sync\\connection.py\", line 122, in _connect\n",
      "    stream = self._network_backend.connect_tcp(**kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\satish.hiremath\\Documents\\GitHub\\crewAI\\myenv\\Lib\\site-packages\\httpcore\\_backends\\sync.py\", line 205, in connect_tcp\n",
      "    with map_exceptions(exc_map):\n",
      "  File \"C:\\Users\\satish.hiremath\\AppData\\Local\\anaconda3\\Lib\\contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(value)\n",
      "  File \"c:\\Users\\satish.hiremath\\Documents\\GitHub\\crewAI\\myenv\\Lib\\site-packages\\httpcore\\_exceptions.py\", line 14, in map_exceptions\n",
      "    raise to_exc(exc) from exc\n",
      "httpcore.ConnectError: [WinError 10061] No connection could be made because the target machine actively refused it\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\satish.hiremath\\Documents\\GitHub\\crewAI\\myenv\\Lib\\site-packages\\openai\\_base_client.py\", line 973, in _request\n",
      "    response = self._client.send(\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\satish.hiremath\\Documents\\GitHub\\crewAI\\myenv\\Lib\\site-packages\\httpx\\_client.py\", line 926, in send\n",
      "    response = self._send_handling_auth(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\satish.hiremath\\Documents\\GitHub\\crewAI\\myenv\\Lib\\site-packages\\httpx\\_client.py\", line 954, in _send_handling_auth\n",
      "    response = self._send_handling_redirects(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\satish.hiremath\\Documents\\GitHub\\crewAI\\myenv\\Lib\\site-packages\\httpx\\_client.py\", line 991, in _send_handling_redirects\n",
      "    response = self._send_single_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\satish.hiremath\\Documents\\GitHub\\crewAI\\myenv\\Lib\\site-packages\\httpx\\_client.py\", line 1027, in _send_single_request\n",
      "    response = transport.handle_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\satish.hiremath\\Documents\\GitHub\\crewAI\\myenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 235, in handle_request\n",
      "    with map_httpcore_exceptions():\n",
      "  File \"C:\\Users\\satish.hiremath\\AppData\\Local\\anaconda3\\Lib\\contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(value)\n",
      "  File \"c:\\Users\\satish.hiremath\\Documents\\GitHub\\crewAI\\myenv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 89, in map_httpcore_exceptions\n",
      "    raise mapped_exc(message) from exc\n",
      "httpx.ConnectError: [WinError 10061] No connection could be made because the target machine actively refused it\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\satish.hiremath\\AppData\\Local\\Temp\\ipykernel_29056\\1463134922.py\", line 4, in <module>\n",
      "    crew.kickoff(inputs={'topic':'AI vs ML vs DL vs Data Science'})\n",
      "  File \"c:\\Users\\satish.hiremath\\Documents\\GitHub\\crewAI\\myenv\\Lib\\site-packages\\crewai\\crew.py\", line 469, in kickoff\n",
      "    result = self._run_sequential_process()\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\satish.hiremath\\Documents\\GitHub\\crewAI\\myenv\\Lib\\site-packages\\crewai\\crew.py\", line 577, in _run_sequential_process\n",
      "    return self._execute_tasks(self.tasks)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\satish.hiremath\\Documents\\GitHub\\crewAI\\myenv\\Lib\\site-packages\\crewai\\crew.py\", line 665, in _execute_tasks\n",
      "    task_output = task.execute_sync(\n",
      "                  ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\satish.hiremath\\Documents\\GitHub\\crewAI\\myenv\\Lib\\site-packages\\crewai\\task.py\", line 180, in execute_sync\n",
      "    return self._execute_core(agent, context, tools)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\satish.hiremath\\Documents\\GitHub\\crewAI\\myenv\\Lib\\site-packages\\crewai\\task.py\", line 234, in _execute_core\n",
      "    result = agent.execute_task(\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\satish.hiremath\\Documents\\GitHub\\crewAI\\myenv\\Lib\\site-packages\\crewai\\agent.py\", line 182, in execute_task\n",
      "    memory = contextual_memory.build_context_for_task(task, context)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\satish.hiremath\\Documents\\GitHub\\crewAI\\myenv\\Lib\\site-packages\\crewai\\memory\\contextual\\contextual_memory.py\", line 24, in build_context_for_task\n",
      "    context.append(self._fetch_stm_context(query))\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\satish.hiremath\\Documents\\GitHub\\crewAI\\myenv\\Lib\\site-packages\\crewai\\memory\\contextual\\contextual_memory.py\", line 33, in _fetch_stm_context\n",
      "    stm_results = self.stm.search(query)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\satish.hiremath\\Documents\\GitHub\\crewAI\\myenv\\Lib\\site-packages\\crewai\\memory\\short_term\\short_term_memory.py\", line 33, in search\n",
      "    return self.storage.search(query=query, score_threshold=score_threshold)  # type: ignore # BUG? The reference is to the parent class, but the parent class does not have this parameters\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\satish.hiremath\\Documents\\GitHub\\crewAI\\myenv\\Lib\\site-packages\\crewai\\memory\\storage\\rag_storage.py\", line 102, in search\n",
      "    else self.app.search(query, limit)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\satish.hiremath\\Documents\\GitHub\\crewAI\\myenv\\Lib\\site-packages\\embedchain\\embedchain.py\", line 699, in search\n",
      "    return [{\"context\": c[0], \"metadata\": c[1]} for c in self.db.query(**params)]\n",
      "                                                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\satish.hiremath\\Documents\\GitHub\\crewAI\\myenv\\Lib\\site-packages\\embedchain\\vectordb\\chroma.py\", line 220, in query\n",
      "    result = self.collection.query(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\satish.hiremath\\Documents\\GitHub\\crewAI\\myenv\\Lib\\site-packages\\chromadb\\api\\models\\Collection.py\", line 327, in query\n",
      "    valid_query_embeddings = self._embed(input=valid_query_texts)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\satish.hiremath\\Documents\\GitHub\\crewAI\\myenv\\Lib\\site-packages\\chromadb\\api\\models\\Collection.py\", line 633, in _embed\n",
      "    return self._embedding_function(input=input)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\satish.hiremath\\Documents\\GitHub\\crewAI\\myenv\\Lib\\site-packages\\chromadb\\api\\types.py\", line 193, in __call__\n",
      "    result = call(self, input)\n",
      "             ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\satish.hiremath\\Documents\\GitHub\\crewAI\\myenv\\Lib\\site-packages\\chromadb\\utils\\embedding_functions.py\", line 188, in __call__\n",
      "    embeddings = self._client.create(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\satish.hiremath\\Documents\\GitHub\\crewAI\\myenv\\Lib\\site-packages\\openai\\resources\\embeddings.py\", line 114, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\satish.hiremath\\Documents\\GitHub\\crewAI\\myenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1260, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\satish.hiremath\\Documents\\GitHub\\crewAI\\myenv\\Lib\\site-packages\\openai\\_base_client.py\", line 937, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\satish.hiremath\\Documents\\GitHub\\crewAI\\myenv\\Lib\\site-packages\\openai\\_base_client.py\", line 997, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\satish.hiremath\\Documents\\GitHub\\crewAI\\myenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\satish.hiremath\\Documents\\GitHub\\crewAI\\myenv\\Lib\\site-packages\\openai\\_base_client.py\", line 997, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\satish.hiremath\\Documents\\GitHub\\crewAI\\myenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1075, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\satish.hiremath\\Documents\\GitHub\\crewAI\\myenv\\Lib\\site-packages\\openai\\_base_client.py\", line 1007, in _request\n",
      "    raise APIConnectionError(request=request) from err\n",
      "openai.APIConnectionError: Connection error.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import traceback\n",
    "try:\n",
    "    crew.kickoff(inputs={'topic':'AI vs ML vs DL vs Data Science'})\n",
    "except Exception as e:\n",
    "    print(e,sys)\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'myenv (Python 3.12.4)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import requests\n",
    "response = requests.get(\"https://www.google.com\")\n",
    "print(response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
